{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"load data.\"\"\"\n",
    "    data = np.loadtxt(\"dataEx3.csv\", delimiter=\",\", skiprows=1, unpack=True)\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    return x, y\n",
    "\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation and Bias-Variance decomposition\n",
    "Implementing 4-fold cross-validation below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "**TODO What does the figure say?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    w = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, w))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, tx_te, w))\n",
    "    return loss_tr, loss_te,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo(degree:int=7,\n",
    "                          k_fold:int=4):\n",
    "    # seed for reproducibility\n",
    "    seed = 12\n",
    "\n",
    "    print(f\"Polynomial degree:{degree}\")\n",
    "    \n",
    "    # lambda search space\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "\n",
    "    # cross validation\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te,_ = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of the best model among various degrees:\n",
    "**TODO: why do we need to find the best lambda for each degree?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_degree_selection(degrees, k_fold, lambdas, seed = 1):\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    #for each degree, we compute the best lambdas and the associated rmse\n",
    "    best_lambdas = []\n",
    "    best_rmses = []\n",
    "    #vary degree\n",
    "    for degree in degrees:\n",
    "        # cross validation\n",
    "        rmse_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                _, loss_te,_ = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            rmse_te.append(np.mean(rmse_te_tmp)) # appending the mean over test\n",
    "        \n",
    "        ind_lambda_opt = np.argmin(rmse_te)\n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_rmses.append(rmse_te[ind_lambda_opt])\n",
    "        \n",
    "    ind_best_degree =  np.argmin(best_rmses)      \n",
    "        \n",
    "    return degrees[ind_best_degree]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "best_degree_selection(degrees=np.arange(2,11), \n",
    "                      k_fold=4, \n",
    "                      lambdas=np.logspace(-4, 0, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box-plot of the RMSE using the cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 20)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    variances = []\n",
    "    # cross validation\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te,_ = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(rmse_tr_tmp)\n",
    "        rmse_te.append(rmse_te_tmp)\n",
    "        variances.append(loss_te)\n",
    "\n",
    "    plt.boxplot(rmse_te)\n",
    "    plt.ylabel('RMSE test')\n",
    "    plt.xlabel('lambdas (Increasing)')\n",
    "    # plt.xticks(ticks=range(len(lambdas)))\n",
    "    \n",
    "    print('Lambdas:',dict(zip(range(1,1+len(lambdas)),lambdas)))\n",
    "    \n",
    "\n",
    "extended_cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "Visualize bias-variance trade-off by implementing the function `bias_variance_demo()`.\n",
    "\n",
    "**TODO: What do you observe? How do you make sense of it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        # split data with a specific seed\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, ratio_train, seed)\n",
    "        \n",
    "        for index_degree, degree in enumerate(degrees):\n",
    "            # form polynomial data\n",
    "            tx_tr = build_poly(x_tr, degree)\n",
    "            tx_te = build_poly(x_te, degree)\n",
    "            # least square\n",
    "            w = least_squares(y_tr, tx_tr)\n",
    "            # calculate the rmse for train and test\n",
    "            rmse_tr[index_seed, index_degree] = np.sqrt(2 * compute_mse(y_tr, tx_tr, w))\n",
    "            rmse_te[index_seed, index_degree] = np.sqrt(2 * compute_mse(y_te, tx_te, w))\n",
    "\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the test error for Least square (i.e. linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_error_distribution_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(50)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        # split data with a specific seed\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, ratio_train, seed)\n",
    "        \n",
    "        for index_degree, degree in enumerate(degrees):\n",
    "            # form polynomial data\n",
    "            tx_tr = build_poly(x_tr, degree)\n",
    "            tx_te = build_poly(x_te, degree)\n",
    "            # least square\n",
    "            w = least_squares(y_tr, tx_tr)\n",
    "            # calculate the rmse for test\n",
    "            rmse_te[index_seed, index_degree] = np.sqrt(2 * compute_mse(y_te, tx_te, w))\n",
    "\n",
    "    plt.boxplot(rmse_te)\n",
    "    plt.title(\"error distribution\")\n",
    "    plt.xlabel(\"degrees\")\n",
    "    plt.ylabel(\"test_error\")\n",
    "test_error_distribution_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ridge regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_distribution_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(50)\n",
    "    num_data = 10000\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        # split data with a specific seed\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, ratio_train, seed)\n",
    "        \n",
    "        k_fold = 4\n",
    "        k_indices = build_k_indices(y_tr, k_fold, 1)\n",
    "        \n",
    "        for index_degree, degree in enumerate(degrees):\n",
    "            # form polynomial data\n",
    "            tx_tr = build_poly(x_tr, degree)\n",
    "            tx_te = build_poly(x_te, degree)\n",
    "            \n",
    "            \n",
    "            # selecting best lambda\n",
    "            rmse_te_tmp = []\n",
    "            ws = []\n",
    "            for lambda_ in lambdas:\n",
    "                rmse_te_tmp2 = []\n",
    "                ws_tmp = []\n",
    "                for k in range(k_fold):\n",
    "                    _, loss_te, w = cross_validation(y_tr, x_tr, k_indices, k, lambda_, degree)\n",
    "                    ws_tmp.append(w)\n",
    "                    rmse_te_tmp2.append(loss_te)\n",
    "                rmse_te_tmp.append(np.mean(rmse_te_tmp2))\n",
    "                ws.append(ws_tmp)\n",
    "            ind_lambda_opt = np.argmin(rmse_te_tmp)\n",
    "            ws_opt = ws[ind_lambda_opt]\n",
    "            lambda_opt = lambdas[ind_lambda_opt]\n",
    "            \n",
    "            # calculate the rmse for test\n",
    "            rmse_te[index_seed, index_degree] =  np.mean([np.sqrt(2 * compute_mse(y_te, tx_te, w)) for w in ws_opt])\n",
    "\n",
    "    plt.boxplot(rmse_te)\n",
    "    plt.title(\"error distribution\")\n",
    "    plt.xlabel(\"degrees\")\n",
    "    plt.ylabel(\"test_error\")\n",
    "test_error_distribution_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
